{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: Implementation of Python Code\n",
    "This appendix provides the implementation details for the Deep Neural Network (DNN) model and data standardization workflows used in this study. The Python code was written using the TensorFlow and Keras libraries for constructing and training the DNN, along with NumPy and Pandas for preprocessing.\n",
    "\n",
    "## A.1 Data Standardization\n",
    "Data standardization is a crucial step in machine learning to ensure all features contribute equally to the training process. The following Python code illustrates the standardization process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming the datasets below are available\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# X: Feature matrix (e.g., Depth, SeisVp, Temperature, etc.)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# y: Target variable (e.g., pore pressure)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Standardizing features\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m X_standardized \u001b[38;5;241m=\u001b[39m (\u001b[43mX\u001b[49m \u001b[38;5;241m-\u001b[39m X\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Standardizing target\u001b[39;00m\n\u001b[1;32m     12\u001b[0m y_standardized \u001b[38;5;241m=\u001b[39m (y \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m/\u001b[39m y\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the datasets below are available\n",
    "# X: Feature matrix (e.g., Depth, SeisVp, Temperature, etc.)\n",
    "# y: Target variable (e.g., pore pressure)\n",
    "\n",
    "# Standardizing features\n",
    "X_standardized = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Standardizing target\n",
    "y_standardized = (y - y.mean(axis=0)) / y.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization ensures that all input features have a mean of 0 and a standard deviation of 1, which significantly improves the convergence speed and accuracy of the DNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2 Deep Neural Network (DNN) Implementation\n",
    "The Deep Neural Network (DNN) model used in this study is implemented as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the input dimension (number of features)\n",
    "input_dim = X_standardized.shape[1] # Number of columns in feature matrix\n",
    "\n",
    "# Create the DNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add input and hidden layers\n",
    "model.add(Dense(64, input_dim=input_dim, activation='relu')) # First layer\n",
    "model.add(Dense(16, activation='relu')) # Second layer\n",
    "model.add(Dense(1)) # Single output for pore pressure prediction\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model with 10 epochs\n",
    "model.fit(X_standardized, y_standardized, epochs=10, batch_size=10)\n",
    "\n",
    "# Predict using the trained model\n",
    "y_pred_standardized = model.predict(X_standardized)\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "y_pred = y_pred_standardized * y.std(axis=0) + y.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3. Key Notes:\n",
    "1. Activation Function: ReLU (Rectified Linear Unit) was used for hidden layers to introduce nonlinearity.\n",
    "2. Optimizer: Adam optimizer was chosen for its ability to dynamically adjust learning rates, improving convergence speed.\n",
    "3. Loss Function: Mean Squared Error (MSE) was used to minimize prediction errors.\n",
    "4. Feature Scaling Impact: Without standardization, the training process requires significantly more epochs and may converge to suboptimal solutions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
